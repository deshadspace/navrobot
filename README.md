# Robot MLOps Autonomy ğŸ¤–

Production-ready MLOps + RL pipeline for autonomous navigation. Runs anywhere: local CPU, Colab, or cloud.

## ğŸ¯ Overview

Complete end-to-end system for training autonomous navigation agents:
- **Simulation**: Generate diverse training data (GridWorld environment)
- **Perception**: Vision models for object detection/segmentation
- **Navigation**: RL policies (PPO/SAC) for collision-free navigation
- **Serving**: FastAPI inference service
- **Monitoring**: Automatic drift detection & retraining

Designed for **real MLOps teams** â€” not just toy examples.

## ğŸ“‹ Table of Contents

- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Phases](#phases)
- [Installation](#installation)
- [Usage](#usage)

## ğŸš€ Quick Start

### Local Setup (5 min)

```bash
# Clone repo
git clone https://github.com/deshadspace/navrobot
cd navrobot

# Create environment
python -m venv venv
source venv/bin/activate  # or: venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Generate data
python simulation/generate_data.py --num_episodes 100

# Train perception model
python perception/train.py --epochs 50

# Train navigation policy
python navigation/train_rl.py --algorithm PPO --total_timesteps 100000

# Start API
uvicorn serving.app:app --reload
```

## ğŸ“ Project Structure

```
robot-mlops-autonomy/
â”œâ”€â”€ configs/                   # Configuration files (YAML)
â”‚   â”œâ”€â”€ env.yaml              # Simulation config
â”‚   â”œâ”€â”€ model.yaml            # Model hyperparameters
â”‚   â”œâ”€â”€ training.yaml         # Training pipeline
â”‚   â””â”€â”€ serving.yaml          # API config
â”œâ”€â”€ data/                      # Data directory (DVC-tracked)
â”‚   â”œâ”€â”€ raw/                  # Original simulation data
â”‚   â”œâ”€â”€ processed/            # Cleaned + resized data
â”‚   â””â”€â”€ features/             # Embeddings + state vectors
â”œâ”€â”€ simulation/                #  Phase 1: Data Factory
â”œâ”€â”€ perception/                #  Phase 2: Vision ML
â”œâ”€â”€ navigation/                #  Phase 3: RL Decision Making
â”œâ”€â”€ pipelines/                 #  Phase 4: MLOps Orchestration
â”œâ”€â”€ serving/                   #  Phase 5: Production API
â”œâ”€â”€ monitoring/                #  Phase 6: Trust & Safety
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ experiments/               # Notebooks + results
â”œâ”€â”€ scripts/                   # Utility scripts
â”œâ”€â”€ ci_cd/                     # GitHub Actions workflows
â”œâ”€â”€ pyproject.toml            # Project metadata
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸ§­ Phases

**Phase 1: Simulation & Data** (Week 1)
```bash
python simulation/generate_data.py
```

**Phase 2: Perception Training** (Week 2)
```bash
python perception/train.py --epochs 50
```

**Phase 3: Navigation with RL** (Week 3)
```bash
python navigation/train_rl.py --algorithm PPO --total_timesteps 1000000
```

**Phase 4: Pipelines & MLOps** (Week 4)
```bash
python pipelines/training_pipeline.py
```

**Phase 5: Serving** (Week 5)
```bash
uvicorn serving.app:app --reload
```

**Phase 6: Monitoring** (Week 6)
```bash
python -c "from pipelines.retrain_pipeline import RetrainPipeline; RetrainPipeline().run()"
```

## ğŸ’» Installation

### Requirements
- Python â‰¥ 3.10
- pip or conda
- ~2GB disk space

### From Source

```bash
git clone https://github.com/deshadspace/navrobot
cd navrobot
pip install -r requirements.txt
```

## ğŸ“– Usage

### Generate Simulation Data

```python
from simulation.envs.gridworld import GridWorldEnv

env = GridWorldEnv(grid_size=32)
obs, info = env.reset()

for step in range(500):
    action = env.action_space.sample()
    obs, reward, done, truncated, info = env.step(action)
    if done:
        obs, _ = env.reset()
```

### Train Perception Model

```bash
python perception/train.py --epochs 50 --batch_size 32
```

### Train RL Policy

```python
from stable_baselines3 import PPO
from navigation.env_wrapper import RLEnvironmentWrapper
from simulation.envs.gridworld import GridWorldEnv

env = GridWorldEnv()
env = RLEnvironmentWrapper(env)

model = PPO("MlpPolicy", env, learning_rate=3e-4)
model.learn(total_timesteps=1000000)
model.save("checkpoints/policy")
```

### Deploy API

```bash
uvicorn serving.app:app --reload
# API at http://localhost:8000
# Docs at http://localhost:8000/docs
```

## ğŸ“š Documentation

- [Simulation README](simulation/README.md) - Environment and data generation
- [Perception README](perception/README.md) - Vision models and training
- [Navigation README](navigation/README.md) - RL policies and algorithms
- [Monitoring README](monitoring/README.md) - Drift detection and retraining

##  Contributing

Contributions welcome! Please fork and submit a pull request.

##  License

MIT License

##  Acknowledgments

Built with PyTorch, Stable-Baselines3, FastAPI, MLflow, and Gymnasium.

---

**Made with â¤ï¸ for the robotics & MLOps community**
