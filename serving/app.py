"""
FastAPI serving application.

Production-ready inference service for models.
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import numpy as np
import yaml
from pathlib import Path
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============= Schemas =============

class PredictionRequest(BaseModel):
    """Request schema for predictions."""
    image_base64: str
    model: str = "perception"  # or "navigation"


class PredictionResponse(BaseModel):
    """Response schema for predictions."""
    model: str
    predictions: List[float]
    confidence: float
    latency_ms: float


class HealthResponse(BaseModel):
    """Health check response."""
    status: str
    models_loaded: List[str]


# ============= Application =============

app = FastAPI(
    title="Robot Autonomy API",
    description="Real-time perception + navigation inference",
    version="1.0.0",
)

# Load config
CONFIG_PATH = Path("configs/serving.yaml")
with open(CONFIG_PATH) as f:
    config = yaml.safe_load(f)

# Global state
models = {}
is_ready = False


@app.on_event("startup")
async def startup_event():
    """Load models on startup."""
    global is_ready, models
    
    logger.info("üöÄ Loading models...")
    
    try:
        # Load perception model
        from perception.models import CNNBaseline
        import torch
        
        perception_model = CNNBaseline(num_classes=10)
        # Load weights: perception_model.load_state_dict(torch.load(...))
        models["perception"] = perception_model
        
        # Load navigation model
        from navigation.policy import PolicyInference
        
        navigation_policy = PolicyInference(Path("checkpoints/policy.zip"))
        models["navigation"] = navigation_policy
        
        is_ready = True
        logger.info("‚úì All models loaded")
    
    except Exception as e:
        logger.error(f"‚ùå Failed to load models: {e}")
        is_ready = False


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    return HealthResponse(
        status="ready" if is_ready else "loading",
        models_loaded=list(models.keys()),
    )


@app.post("/predict/perception", response_model=PredictionResponse)
async def predict_perception(request: PredictionRequest):
    """
    Perception inference endpoint.
    
    Args:
        request: prediction request with base64 image
        
    Returns:
        predictions with confidence and latency
    """
    import time
    import base64
    from io import BytesIO
    from PIL import Image
    import torch
    
    if not is_ready:
        raise HTTPException(status_code=503, detail="Models not loaded")
    
    start_time = time.time()
    
    try:
        # Decode image
        image_data = base64.b64decode(request.image_base64)
        image = Image.open(BytesIO(image_data))
        image_array = np.array(image).astype(np.float32) / 255.0
        image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).unsqueeze(0)
        
        # Inference
        with torch.no_grad():
            output = models["perception"](image_tensor)
            logits = output.numpy()[0]
            predictions = logits.tolist()
            confidence = float(np.exp(logits).max() / np.exp(logits).sum())
        
        latency_ms = (time.time() - start_time) * 1000
        
        return PredictionResponse(
            model="perception",
            predictions=predictions,
            confidence=confidence,
            latency_ms=latency_ms,
        )
    
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/predict/navigation", response_model=PredictionResponse)
async def predict_navigation(request: PredictionRequest):
    """
    Navigation policy inference endpoint.
    
    Args:
        request: prediction request with base64 image
        
    Returns:
        action with confidence and latency
    """
    import time
    import base64
    from io import BytesIO
    from PIL import Image
    
    if not is_ready:
        raise HTTPException(status_code=503, detail="Models not loaded")
    
    start_time = time.time()
    
    try:
        # Decode image
        image_data = base64.b64decode(request.image_base64)
        image = Image.open(BytesIO(image_data))
        image_array = np.array(image).astype(np.uint8)
        
        # Inference
        action = models["navigation"].predict(image_array, deterministic=True)
        
        latency_ms = (time.time() - start_time) * 1000
        
        return PredictionResponse(
            model="navigation",
            predictions=action.tolist(),
            confidence=1.0,  # placeholder
            latency_ms=latency_ms,
        )
    
    except Exception as e:
        logger.error(f"Policy inference error: {e}")
        raise HTTPException(status_code=400, detail=str(e))


@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint."""
    # Placeholder: return Prometheus-formatted metrics
    return {
        "predictions_total": 1000,
        "prediction_latency_ms": 15.5,
    }


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        app,
        host=config["api"]["host"],
        port=config["api"]["port"],
        workers=1,
    )
