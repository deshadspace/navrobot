# Serving & API Configuration

api:
  host: "0.0.0.0"
  port: 8000
  debug: false
  reload: false
  
  title: "Robot Autonomy API"
  description: "Real-time perception + navigation inference service"
  version: "1.0.0"

models:
  perception:
    model_uri: "models://perception-model/production"  # MLflow URI
    backend: "torch"
    device: "cpu"
    batch_size: 8
    
  navigation:
    model_uri: "models://navigation-policy/production"
    backend: "onnx"  # lightweight format
    device: "cpu"

inference:
  timeout: 30  # seconds
  max_batch_size: 32
  
  preprocessing:
    image_size: [224, 224]
    normalize: true
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  
  postprocessing:
    nms_threshold: 0.5
    confidence_threshold: 0.5

cache:
  enabled: true
  max_size: 1000
  ttl: 3600  # seconds

monitoring:
  enabled: true
  log_predictions: true
  log_latency: true
  metrics_endpoint: "/metrics"
  
  # Prometheus
  prometheus:
    enabled: true
    namespace: "robot_api"

docker:
  base_image: "python:3.11-slim"
  cuda_enabled: false
